{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.chdir('../data')\n",
    "#os.chdir('C:\\\\Users\\\\wangd\\\\git\\\\lets_learn\\\\infclean\\\\data')\n",
    "\n",
    "def dump_history(hist, filename):\n",
    "    json.dump(hist.history, open(filename, 'w'))\n",
    "    \n",
    "def load_history(filename):\n",
    "    return json.load(open(filename, 'r'))\n",
    "\n",
    "def load_dataset(train_path, valid_path, categorical_attributes, omit_attributes, target_attr):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    values = np.concatenate(([train_df[cat_attr].astype(str).unique() \n",
    "                              for cat_attr in categorical_attributes]), axis=None)\n",
    "    keys = pd.Categorical(values).codes\n",
    "    category_dict = dict(zip(keys, values))\n",
    "    train_df[categorical_attributes] = train_df[categorical_attributes].astype(str).replace(values, keys)\n",
    "    valid_df[categorical_attributes] = valid_df[categorical_attributes].astype(str).replace(values, keys)\n",
    "    for o in omit_attributes:\n",
    "        train_df.pop(o)\n",
    "        valid_df.pop(o)\n",
    "    train_target = train_df.pop(target_attr)\n",
    "    valid_target = valid_df.pop(target_attr)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_df.values, train_target.values))\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_df.values, valid_target.values))\n",
    "    train_dataset = train_dataset.shuffle(len(train_df)).batch(1)\n",
    "    valid_dataset = valid_dataset.shuffle(len(valid_df)).batch(1)\n",
    "    vocab_size = len(values)\n",
    "    return train_dataset, valid_dataset, category_dict, vocab_size\n",
    "\n",
    "print('TENSORFLOW VERSION: {}'.format(tf.__version__))\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('NO GPU FOUND')\n",
    "else:\n",
    "    print('DEFAULT GPU DEVICE: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Categorical Features\n",
    "- A feed-forward NN that learns embedding vectors\n",
    "- Model derived from word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word2Vec\n",
    "Given a text corpus, it learns word vectors such that words that share similar context are located closer in the vector space.\n",
    "![title](../resource/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Our model\n",
    "Given observation dataset, it learns vector representations for categorical columns, where categorical values that co-occur frequently get similar vector representations. Each categorical column has its own vocabulary.\n",
    "![title](../resource/w2v_multi_vocab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_multi_vocab_model(input_length, voc_size, e_dim):\n",
    "    assert input_length > 1, 'Input length must be greater than 1, current: %i' % input_length \n",
    "    word_input_list = []\n",
    "    word_input_name = 'word_input_{}'\n",
    "    for i in range(input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i))\n",
    "        word_input_list.append(input_i)\n",
    "    numerical_input = keras.layers.Input(shape=(1, ), name='numerical_input')\n",
    "    \n",
    "    word_encode_list = []\n",
    "    word_encode_name = 'encode_reshape_{}'\n",
    "    for i in range(input_length):\n",
    "        encode_i = keras.layers.Embedding(input_dim=voc_size[i],\n",
    "                                          output_dim=e_dim,\n",
    "                                          input_length=1)(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_encode_name.format(i))(encode_i)\n",
    "        word_encode_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(input_length-1):\n",
    "        for j in range(i+1, input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_encode_list[i], word_encode_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         name=dot_sim_name.format(i, j))\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, numerical_input], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.append(numerical_input)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='cbow_model')\n",
    "    #m.summary()\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    keras.utils.plot_model(m, to_file='w2v_multi_vocab.png')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['dirty_state',\n",
    "                    'dirty_city',\n",
    "                    'dirty_zip']\n",
    "TARGET = 'rent'\n",
    "OMIT = ['intended_state', 'intended_city', 'intended_zip']\n",
    "TRAIN_FILENAME = 'rent_data_low_error.csv'\n",
    "VALID_FILENAME = 'rent_data_low_error_validation.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Load dataset into one vocab for all columns, integer-encoded\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "valid_df = pd.read_csv(VALID_FILENAME)\n",
    "'''unique_values = np.concatenate(([train_df[cat_attr].astype(str).unique()\n",
    "                                 for cat_attr in CATEGORICAL_ATTR]), axis=None)\n",
    "keys = pd.Categorical(unique_values).codes\n",
    "category_dict = dict(zip(keys, unique_values))\n",
    "train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "valid_df[CATEGORICAL_ATTR] = valid_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "for o in OMIT:\n",
    "    train_df.pop(o)\n",
    "    valid_df.pop(o)'''\n",
    "    \n",
    "# Load dataset into separate vocabularies, integer-encoded\n",
    "unique_values = [train_df[cat_attr].astype(str).unique() for cat_attr in CATEGORICAL_ATTR]\n",
    "keys = [pd.Categorical(u).codes for u in unique_values]\n",
    "category_dict = []\n",
    "for i in range(len(keys)):\n",
    "    category_dict.append(dict(zip(keys[i], unique_values[i])))\n",
    "    train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values[i], keys[i])\n",
    "for o in OMIT:\n",
    "    train_df.pop(o)\n",
    "    valid_df.pop(o)\n",
    "# Generate labels, i.e. we label all observations as positive samples\n",
    "train_target = np.ones(train_df.shape[0])\n",
    "valid_target = np.ones(valid_df.shape[0])\n",
    "\n",
    "# Load data into model, single vocab\n",
    "# vocabulary_size = len(unique_values)\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(vocabulary_size ** 0.25))\n",
    "# model = get_w2v_model(len(CATEGORICAL_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "\n",
    "# multi vocab\n",
    "vocabulary_size = [len(u) for u in unique_values]\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(sum(vocabulary_size)**0.25))\n",
    "adaptive_embedding_dim = 10\n",
    "model = get_w2v_multi_vocab_model(len(CATEGORICAL_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "train_np = [train_df[cat_attr].to_numpy() for cat_attr in CATEGORICAL_ATTR]\n",
    "train_np.append(train_df[TARGET].to_numpy())\n",
    "\n",
    "history = model.fit(x=train_np,\n",
    "                    y=train_target,\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save embeddings from one vocab\n",
    "'''e = model.layers[3]\n",
    "weights = e.get_weights()[0]\n",
    "out_v = io.open('w2v_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('w2v_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(0, vocabulary_size):\n",
    "    vec = weights[i]\n",
    "    out_m.write(category_dict[i] + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Save for multiple vocabs\n",
    "tsv_name = '{}_{}.tsv'\n",
    "for i, cat_attr in enumerate(CATEGORICAL_ATTR, start=0):\n",
    "    e = model.layers[3 + i]\n",
    "    weights = e.get_weights()[0]\n",
    "    out_v = io.open(tsv_name.format(cat_attr, 'vec'), 'w', encoding='utf-8')\n",
    "    out_m = io.open(tsv_name.format(cat_attr, 'meta'), 'w', encoding='utf-8')\n",
    "    \n",
    "    for j in range(0, vocabulary_size[i]):\n",
    "        vec = weights[j]\n",
    "        out_m.write(category_dict[i][j] + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "'''\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results\n",
    "![title](../resource/city_emb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model clearly learns that cities from the same state are \"similar\"\n",
    "\n",
    "- Erlangen, Ingolstadt and Freising are the most Bavarian cities except MÃ¼nchen\n",
    "\n",
    "    $\\to$ Rent is incorperated into the notion of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Plug embeddings into generative model and evaluate performance change\n",
    "- Think about whether negative sampling could improve learning results\n",
    "- Reason about the learning results and the meaning of similarity\n",
    "- Check out other possible models for embedding learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
