{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#os.chdir('/Users/wang/Documents/git/lets_learn/infclean/data')\n",
    "os.chdir('C:/git/lets_learn/infclean/data')\n",
    "#os.chdir('C:/Users/wangd/git/lets_learn/infclean/data')\n",
    "\n",
    "def dump_history(hist, filename):\n",
    "    json.dump(hist.history, open(filename, 'w'))\n",
    "    \n",
    "def load_history(filename):\n",
    "    return json.load(open(filename, 'r'))\n",
    "\n",
    "\n",
    "def load_dataset(train_path, valid_path, categorical_attributes, omit_attributes, target_attr):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    valid_df = pd.read_csv(valid_path)\n",
    "    values = np.concatenate(([train_df[cat_attr].astype(str).unique() \n",
    "                              for cat_attr in categorical_attributes]), axis=None)\n",
    "    keys = pd.Categorical(values).codes\n",
    "    category_dict = dict(zip(keys, values))\n",
    "    train_df[categorical_attributes] = train_df[categorical_attributes].astype(str).replace(values, keys)\n",
    "    valid_df[categorical_attributes] = valid_df[categorical_attributes].astype(str).replace(values, keys)\n",
    "    for o in omit_attributes:\n",
    "        train_df.pop(o)\n",
    "        valid_df.pop(o)\n",
    "    train_target = train_df.pop(target_attr)\n",
    "    valid_target = valid_df.pop(target_attr)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_df.values, train_target.values))\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_df.values, valid_target.values))\n",
    "    train_dataset = train_dataset.shuffle(len(train_df)).batch(1)\n",
    "    valid_dataset = valid_dataset.shuffle(len(valid_df)).batch(1)\n",
    "    vocab_size = len(values)\n",
    "    return train_dataset, valid_dataset, category_dict, vocab_size\n",
    "\n",
    "def negative_sampling(df, target_attr, ignore_attrs, n_per_target_attr):\n",
    "    # which cities exist?\n",
    "    target_attr_vals = df[target_attr].astype(str).unique()\n",
    "    all_attrs = list(df)\n",
    "    other_attrs = list(filter(lambda x: x != target_attr, all_attrs))\n",
    "    other_attrs = list(filter(lambda x: x not in ignore_attrs, other_attrs))\n",
    "    positive_samples = [[] for _ in range(len(ignore_attrs) + 1)]\n",
    "    negative_samples = [[] for _ in range(len(other_attrs))]\n",
    "\n",
    "    # for each entry, sample n negative samples with city as target\n",
    "    for (i, target_val) in enumerate(target_attr_vals):\n",
    "        # all entries with this city\n",
    "        target_val_df = df[df[target_attr] == target_val].reset_index()\n",
    "        del target_val_df['index']\n",
    "        neg_attr_vals_list = []\n",
    "        for attr in other_attrs:\n",
    "            pos_attr_vals = target_val_df[attr].unique()\n",
    "            neg_attr_vals = pd.unique(list(filter(lambda x: x not in pos_attr_vals, df[attr])))\n",
    "            if len(neg_attr_vals) == 0:\n",
    "                neg_attr_vals_list.append(pos_attr_vals)\n",
    "            else:\n",
    "                neg_attr_vals_list.append(neg_attr_vals)\n",
    "        # for _ in range(n_per_target_attr * target_val_df.shape[0]):\n",
    "        for _ in range(n_per_target_attr):\n",
    "            # sample negative values for other attrs\n",
    "            for (j, attr) in enumerate(other_attrs):\n",
    "                neg_attr_val = neg_attr_vals_list[j][np.random.randint(0, len(neg_attr_vals_list[j]))]\n",
    "                negative_samples[j].append(neg_attr_val)\n",
    "            positive_samples[0].append(target_val)\n",
    "            # sample positive values for ignored attributes\n",
    "            for (j, attr) in enumerate(ignore_attrs):\n",
    "                positive_samples[j + 1].append(target_val_df[attr][np.random.randint(0, target_val_df.shape[0])])\n",
    "    # [city, ignored_attrs..., other_attrs...]\n",
    "    column_names = np.concatenate((np.concatenate(([target_attr], ignore_attrs)), other_attrs))\n",
    "    samples = np.transpose(np.concatenate((positive_samples, negative_samples)))\n",
    "    negative_samples_df = pd.DataFrame(samples, columns=column_names)\n",
    "    negative_samples_df['good'] = np.zeros(negative_samples_df.shape[0])\n",
    "    return negative_samples_df\n",
    "\n",
    "print('TENSORFLOW VERSION: {}'.format(tf.__version__))\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('NO GPU FOUND')\n",
    "else:\n",
    "    print('DEFAULT GPU DEVICE: {}'.format(tf.test.gpu_device_name()))\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## w2v models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_w2v_single_vocab_model(cat_input_length, num_input_length, voc_size, e_dim):\n",
    "    word_input_list = []\n",
    "    word_input_name = 'word_input_{}'\n",
    "    num_input_list = []\n",
    "    num_input_name = 'num_input_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i))\n",
    "        word_input_list.append(input_i)\n",
    "    for i in range(num_input_length):\n",
    "        numerical_input_i = keras.layers.Input(shape=(1, ), name=num_input_name.format(i))\n",
    "        num_input_list.append(numerical_input_i)\n",
    "    \n",
    "    embedding = keras.layers.Embedding(input_dim=voc_size, \n",
    "                                       output_dim=e_dim, \n",
    "                                       input_length=cat_input_length,\n",
    "                                       name='embedding')\n",
    "    word_reshaped_list = []\n",
    "    word_reshaped_name = 'encode_reshape_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        encoded_i = embedding(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_reshaped_name.format(i))(encoded_i)\n",
    "        word_reshaped_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(cat_input_length-1):\n",
    "        for j in range(i+1, cat_input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_reshaped_list[i], word_reshaped_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         name=dot_sim_name.format(i, j))\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_num = keras.layers.concatenate(num_input_list, axis=1) if len(num_input_list) > 1 else num_input_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, merge_num], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.extend(num_input_list)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='cbow_model')\n",
    "    #m.summary()\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    keras.utils.plot_model(m, to_file='w2v_single_vocab.png')\n",
    "    return m\n",
    "\n",
    "def get_w2v_multi_vocab_model(cat_input_length, num_input_length, voc_size, e_dim):\n",
    "    assert cat_input_length > 1, 'Input length must be greater than 1, current: %i' % cat_input_length \n",
    "    word_input_list = []\n",
    "    cat_names = ['state', 'city', 'zip', 'construct_year']\n",
    "    #word_input_name = 'word_input_{}'\n",
    "    word_input_name = 'cat_attr_{}'\n",
    "    num_input_list = []\n",
    "    num_names = ['living_space', 'rent']\n",
    "    #num_input_name = 'num_input_{}'\n",
    "    num_input_name = 'num_attr_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i+1))\n",
    "        word_input_list.append(input_i)\n",
    "    for i in range(num_input_length):\n",
    "        numerical_input_i = keras.layers.Input(shape=(1, ), name=num_input_name.format(i+1))\n",
    "        num_input_list.append(numerical_input_i)\n",
    "    \n",
    "    word_encode_list = []\n",
    "    word_encode_name = 'encode_reshape_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        encode_i = keras.layers.Embedding(input_dim=voc_size[i],\n",
    "                                          output_dim=e_dim,\n",
    "                                          input_length=1)(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_encode_name.format(i))(encode_i)\n",
    "        word_encode_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(cat_input_length-1):\n",
    "        for j in range(i+1, cat_input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_encode_list[i], word_encode_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         #name=dot_sim_name.format(i, j)\n",
    "                                         )\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_num = keras.layers.concatenate(num_input_list, axis=1) if len(num_input_list) > 1 else num_input_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, merge_num], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.extend(num_input_list)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='embedding_training_model')\n",
    "    m.summary()\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    keras.utils.plot_model(m, to_file='w2v_multi_vocab_tt_3.png', show_shapes=True, show_layer_names=True)\n",
    "    return m\n",
    "    \n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Enriched rent data - Multi vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['state',\n",
    "                    'city',\n",
    "                    'zip',\n",
    "                    'construct_year'\n",
    "                    #'heating_type',\n",
    "                    ]\n",
    "NUMERICAL_ATTR = [#'zip',\n",
    "    'living_space',\n",
    "    'rent']\n",
    "BOOLEAN_ATTR = [#'has_parking',\n",
    "                #'has_balcony',\n",
    "                ]\n",
    "\n",
    "#TRAIN_FILENAME = 'C:/git/lets_learn/infclean/data/rent_data/enriched_rent_data_100000.csv'\n",
    "TRAIN_FILENAME = 'C:/git/lets_learn/infclean/data/rent_data/neo_enriched_rent_30_per_city.csv'\n",
    "#TRAIN_FILENAME = 'C:/git/lets_learn/infclean/data/rent_data/simple_rent_5_per_city.csv'\n",
    "#TRAIN_FILENAME = '/Users/wang/Documents/git/lets_learn/infclean/data/rent_data/NEGSAMP_neo_rent_60_per_city.csv'\n",
    "#EMBEDDING_DIM = 7\n",
    "#TRAIN_FILENAME = 'C:/git/lets_learn/infclean/data/rent_data/NEGSAMP_simple_rent_20_per_city.csv'\n",
    "EMBEDDING_DIM = 4\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "# ignore negative samples from julia\n",
    "#train_df = train_df[train_df['negative_sample'] != 1]\n",
    "train_df = train_df.drop(columns=['negative_sample'])\n",
    "neg_df = negative_sampling(train_df, 'city', [], 5)\n",
    "train_df['good'] = np.ones(train_df.shape[0])\n",
    "train_df = pd.concat([train_df, neg_df], ignore_index=True)\n",
    "# shuffle\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['state',\n",
    "                    'city',\n",
    "                    #'zip',\n",
    "                    #'construct_year',\n",
    "                    #'heating_type',\n",
    "                    ]\n",
    "NUMERICAL_ATTR = [#'zip',\n",
    "    #'living_space',\n",
    "    'rent']\n",
    "unique_values = [train_df[cat_attr].astype(str).unique() for cat_attr in CATEGORICAL_ATTR]\n",
    "keys = [pd.Categorical(u).codes for u in unique_values]\n",
    "category_dict = []\n",
    "for i, key_set in enumerate(keys):\n",
    "    category_dict.append(dict(zip(key_set, unique_values[i])))\n",
    "    train_df[CATEGORICAL_ATTR[i]] = train_df[CATEGORICAL_ATTR[i]].astype(str).replace(unique_values[i], key_set)\n",
    "\n",
    "# multi vocab\n",
    "vocabulary_size = [len(u) for u in unique_values]\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(sum(vocabulary_size)**0.25))\n",
    "adaptive_embedding_dim = 100\n",
    "model = get_w2v_multi_vocab_model(len(CATEGORICAL_ATTR), len(NUMERICAL_ATTR), vocabulary_size, adaptive_embedding_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset into separate vocabularies, integer-encoded\n",
    "unique_values = [train_df[cat_attr].astype(str).unique() for cat_attr in CATEGORICAL_ATTR]\n",
    "keys = [pd.Categorical(u).codes for u in unique_values]\n",
    "category_dict = []\n",
    "for i, key_set in enumerate(keys):\n",
    "    category_dict.append(dict(zip(key_set, unique_values[i])))\n",
    "    train_df[CATEGORICAL_ATTR[i]] = train_df[CATEGORICAL_ATTR[i]].astype(str).replace(unique_values[i], key_set)\n",
    "\n",
    "# multi vocab\n",
    "vocabulary_size = [len(u) for u in unique_values]\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(sum(vocabulary_size)**0.25))\n",
    "adaptive_embedding_dim = EMBEDDING_DIM\n",
    "model = get_w2v_multi_vocab_model(len(CATEGORICAL_ATTR), len(NUMERICAL_ATTR) + len(BOOLEAN_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "\n",
    "# Generate labels, i.e. we label all observations as positive samples\n",
    "# train_target = np.ones(train_df.shape[0])\n",
    "train_target = train_df['good'].to_numpy().astype(int)\n",
    "\n",
    "train_np = [train_df[cat_attr].to_numpy().astype(int) for cat_attr in CATEGORICAL_ATTR]\n",
    "for bool_attr in BOOLEAN_ATTR:\n",
    "    train_np.append(train_df[bool_attr].to_numpy().astype(int))\n",
    "for num_attr in NUMERICAL_ATTR:\n",
    "    train_np.append(train_df[num_attr].to_numpy().astype(float))\n",
    "\n",
    "\n",
    "history = model.fit(x=train_np,\n",
    "                    y=train_target,\n",
    "                    batch_size=32,\n",
    "                    epochs=300)\n",
    "print('SUCCESS')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EK onekey data - Multi vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = [#'catalog_id',\n",
    "                #'article_id',\n",
    "                #'lower_bound',\n",
    "                'unit',\n",
    "                'keywords',\n",
    "                'manufacturer_name',\n",
    "                #'ean',\n",
    "                'set_id',\n",
    "                    ]\n",
    "NUMERICAL_ATTR = ['ek_amount',\n",
    "                #'vk_amount',\n",
    "                    ]\n",
    "\n",
    "#TRAIN_FILENAME = '/Users/wang/Documents/git/lets_learn/infclean/data/mercateo/NEGSAMP_50000_onekey_ek_no_ean.csv'\n",
    "TRAIN_FILENAME = 'C:/git/lets_learn/infclean/data/mercateo/NEGSAMP_catalog_5497.csv'\n",
    "EMBEDDING_DIM = 7\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "train_df = train_df.drop(columns=['negative_sample'])\n",
    "neg_df = negative_sampling(train_df, 'keywords', ['article_id', 'ek_amount'], 5)\n",
    "train_df['good'] = np.ones(train_df.shape[0])\n",
    "train_df = pd.concat([train_df, neg_df], ignore_index=True)\n",
    "# shuffle\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Load dataset into separate vocabularies, integer-encoded\n",
    "unique_values = [train_df[cat_attr].astype(str).unique() for cat_attr in CATEGORICAL_ATTR]\n",
    "keys = [pd.Categorical(u).codes for u in unique_values]\n",
    "category_dict = []\n",
    "for i, key_set in enumerate(keys):\n",
    "    category_dict.append(dict(zip(key_set, unique_values[i])))\n",
    "    train_df[CATEGORICAL_ATTR[i]] = train_df[CATEGORICAL_ATTR[i]].astype(str).replace(unique_values[i], key_set)\n",
    "\n",
    "# multi vocab\n",
    "vocabulary_size = [len(u) for u in unique_values]\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(sum(vocabulary_size)**0.25))\n",
    "adaptive_embedding_dim = EMBEDDING_DIM\n",
    "model = get_w2v_multi_vocab_model(len(CATEGORICAL_ATTR), len(NUMERICAL_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "\n",
    "\n",
    "# Generate labels, i.e. we label all observations as positive samples\n",
    "# train_target = np.ones(train_df.shape[0])\n",
    "train_target = train_df['good'].to_numpy().astype(int)\n",
    "\n",
    "train_np = [train_df[cat_attr].to_numpy().astype(int) for cat_attr in CATEGORICAL_ATTR]\n",
    "for num_attr in NUMERICAL_ATTR:\n",
    "    train_np.append(train_df[num_attr].to_numpy().astype(float))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "history = model.fit(x=train_np,\n",
    "                    y=train_target,\n",
    "                    batch_size=32,\n",
    "                    epochs=50)\n",
    "print('SUCCESS')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vector IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save embeddings from one vocab\n",
    "'''e = model.layers[3]\n",
    "weights = e.get_weights()[0]\n",
    "out_v = io.open('w2v_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('w2v_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(0, vocabulary_size):\n",
    "    vec = weights[i]\n",
    "    out_m.write(category_dict[i] + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()'''\n",
    "\n",
    "# Save for multiple vocabs\n",
    "tsv_name = 'NEGSAMP_catalog_5497_5_col_{}_{}.tsv'\n",
    "for i, cat_attr in enumerate(CATEGORICAL_ATTR, start=0):\n",
    "    e = model.layers[len(CATEGORICAL_ATTR) + i]\n",
    "    weights = e.get_weights()[0]\n",
    "    out_v = io.open(tsv_name.format(cat_attr, 'vec'), 'w', encoding='utf-8')\n",
    "    out_m = io.open(tsv_name.format(cat_attr, 'meta'), 'w', encoding='utf-8')\n",
    "    \n",
    "    for j in range(0, vocabulary_size[i]):\n",
    "        vec = weights[j]\n",
    "        out_m.write(category_dict[i][j] + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "-----------------------------\n",
    "## Enriched rent data - Single vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['state',\n",
    "                    'city',\n",
    "                    'zip',\n",
    "                    'construct_year',\n",
    "                    'heating_type']\n",
    "NUMERICAL_ATTR = ['living_space',\n",
    "                  'rent']\n",
    "BOOLEAN_ATTR = ['has_parking',\n",
    "                'has_balcony']\n",
    "\n",
    "TRAIN_FILENAME = 'enriched_rent_data.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "# Load dataset into one vocab for all columns, integer-encoded\n",
    "unique_values = np.concatenate(([train_df[cat_attr].astype(str).unique()\n",
    "                                 for cat_attr in CATEGORICAL_ATTR]), axis=None)\n",
    "keys = pd.Categorical(unique_values).codes\n",
    "category_dict = dict(zip(keys, unique_values))\n",
    "train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "    \n",
    "# Generate labels, i.e. we label all observations as positive samples\n",
    "train_target = np.ones(train_df.shape[0])\n",
    "\n",
    "# Load data into model, single vocab\n",
    "vocabulary_size = len(unique_values)\n",
    "adaptive_embedding_dim = EMBEDDING_DIM\n",
    "model = get_w2v_single_vocab_model(len(CATEGORICAL_ATTR), len(NUMERICAL_ATTR) + len(BOOLEAN_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "\n",
    "train_np = [train_df[cat_attr].to_numpy().astype(int) for cat_attr in CATEGORICAL_ATTR]\n",
    "for bool_attr in BOOLEAN_ATTR:\n",
    "    train_np.append(train_df[bool_attr].to_numpy().astype(int))\n",
    "for num_attr in NUMERICAL_ATTR:\n",
    "    train_np.append(train_df[num_attr].to_numpy().astype(float))\n",
    "\n",
    "history = model.fit(x=train_np,\n",
    "                    y=train_target,\n",
    "                    batch_size=1,\n",
    "                    epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split single vocab\n",
    "class Category:\n",
    "    key_dict = {}\n",
    "    col = None\n",
    "    \n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "        \n",
    "categories = {}\n",
    "for cat_attr in CATEGORICAL_ATTR:\n",
    "    categories[cat_attr] = Category(cat_attr)\n",
    "    \n",
    "df = pd.read_csv(TRAIN_FILENAME)\n",
    "for key in category_dict:\n",
    "    cur_val = category_dict[key]\n",
    "    for cat_attr in CATEGORICAL_ATTR:\n",
    "        if cur_val in df[cat_attr].to_numpy():\n",
    "            categories[cat_attr].key_dict[key] = cur_val\n",
    "print(\"split finished\")\n",
    "            \n",
    "# Save for multiple vocabs\n",
    "e = model.layers[5]\n",
    "weights = e.get_weights()[0]\n",
    "\n",
    "for cat_attr in CATEGORICAL_ATTR:\n",
    "    cat = categories[cat_attr]\n",
    "    out_v = io.open('{}_vecs.tsv'.format(cat_attr), 'w', encoding='utf-8')\n",
    "    out_m = io.open('{}_meta.tsv'.format(cat_attr), 'w', encoding='utf-8')\n",
    "    for key in cat.key_dict:\n",
    "        vec = weights[key]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(cat.key_dict[key] + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()    \n",
    "\n",
    "print('SUCCESS')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding rent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cbow style model for all columns\n",
    "def get_ek_prediction_model(voc_size, e_dim):\n",
    "    m = tf.keras.Sequential()\n",
    "    m.add(layers.Embedding(voc_size, e_dim))\n",
    "    #m.add(layers.Dense(e_dim, activation='relu'))\n",
    "    m.add(layers.Dense(1, activation='relu'))\n",
    "    #m.summary()\n",
    "\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])\n",
    "    return m\n",
    "\n",
    "\n",
    "CATEGORICAL_ATTR = ['dirty_state',\n",
    "                    'dirty_city',\n",
    "                    'dirty_zip']\n",
    "TARGET = 'rent'\n",
    "OMIT = ['intended_state', 'intended_city', 'intended_zip']\n",
    "TRAIN_FILENAME = 'rent_data_low_error.csv'\n",
    "VALID_FILENAME = 'rent_data_low_error_validation.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Load dataset, separate vocab\n",
    "# df = pd.read_csv(FILENAME)\n",
    "'''for cat_attr in CATEGORICAL_ATTR:\n",
    "    df[cat_attr] = pd.Categorical(df[cat_attr])\n",
    "    df[cat_attr] = df[cat_attr].cat.codes\n",
    "for o in OMIT:\n",
    "    df.pop(o)'''\n",
    "\n",
    "# One vocab for all columns, integer-encoded\n",
    "train_ds, val_ds, cat_encode_dict, vocabulary_size = load_dataset(TRAIN_FILENAME, \n",
    "                                                                  VALID_FILENAME, \n",
    "                                                                  CATEGORICAL_ATTR, \n",
    "                                                                  OMIT, \n",
    "                                                                  TARGET)\n",
    "\n",
    "adaptive_embedding_dim = min(EMBEDDING_DIM, int(vocabulary_size**0.25))\n",
    "model = get_ek_prediction_model(vocabulary_size, adaptive_embedding_dim)\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=10,\n",
    "                    validation_data=val_ds, \n",
    "                    validation_steps=20)\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "out_v = io.open('new_clean_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('new_clean_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(0, vocabulary_size):\n",
    "    vec = weights[i]\n",
    "    out_m.write(cat_encode_dict[i] + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Advanced rent data embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Skip-gram style model, one vocab\n",
    "# https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
    "def get_sg_model(voc_size, e_dim):\n",
    "    pass\n",
    "\n",
    "def get_w2v_multi_vocab_model(cat_input_length, num_input_length, voc_size, e_dim):\n",
    "    assert cat_input_length > 1, 'Input length must be greater than 1, current: %i' % cat_input_length \n",
    "    word_input_list = []\n",
    "    word_input_name = 'word_input_{}'\n",
    "    num_input_list = []\n",
    "    num_input_name = 'num_input_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i))\n",
    "        word_input_list.append(input_i)\n",
    "    for i in range(num_input_length):\n",
    "        numerical_input_i = keras.layers.Input(shape=(1, ), name=num_input_name.format(i))\n",
    "        num_input_list.append(numerical_input_i)\n",
    "    \n",
    "    word_encode_list = []\n",
    "    word_encode_name = 'encode_reshape_{}'\n",
    "    for i in range(cat_input_length):\n",
    "        encode_i = keras.layers.Embedding(input_dim=voc_size[i],\n",
    "                                          output_dim=e_dim,\n",
    "                                          input_length=1)(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_encode_name.format(i))(encode_i)\n",
    "        word_encode_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(cat_input_length-1):\n",
    "        for j in range(i+1, cat_input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_encode_list[i], word_encode_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         name=dot_sim_name.format(i, j))\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_num = keras.layers.concatenate(num_input_list, axis=1) if len(num_input_list) > 1 else num_input_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, merge_num], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.extend(num_input_list)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='cbow_model')\n",
    "    #m.summary()\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "   # keras.utils.plot_model(m, to_file='w2v_multi_vocab.png')\n",
    "    return m\n",
    "\n",
    "# word2vec style model, one vocabulary\n",
    "# input_length ... number of input cat. attributes excl. rent/ek\n",
    "# voc_size ... size of the single vocabulary\n",
    "def get_w2v_model(input_length, voc_size, e_dim):\n",
    "    assert input_length > 1, 'Input length must be greater than 1, current: %i' % input_length \n",
    "    word_input_list = []\n",
    "    word_input_name = 'word_input_{}'\n",
    "    for i in range(input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i))\n",
    "        word_input_list.append(input_i)\n",
    "    numerical_input = keras.layers.Input(shape=(1, ), name='numerical_input')\n",
    "    \n",
    "    embedding = keras.layers.Embedding(input_dim=voc_size, \n",
    "                                       output_dim=e_dim, \n",
    "                                       input_length=input_length,\n",
    "                                       name='embedding')\n",
    "    word_reshaped_list = []\n",
    "    word_reshaped_name = 'encode_reshape_{}'\n",
    "    for i in range(input_length):\n",
    "        encoded_i = embedding(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_reshaped_name.format(i))(encoded_i)\n",
    "        word_reshaped_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(input_length-1):\n",
    "        for j in range(i+1, input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_reshaped_list[i], word_reshaped_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         name=dot_sim_name.format(i, j))\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, numerical_input], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.append(numerical_input)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='cbow_model')\n",
    "    #m.summary()\n",
    "    m.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    keras.utils.plot_model(m, to_file='w2v_one_vocab.png')\n",
    "    return m\n",
    "\n",
    "\n",
    "CATEGORICAL_ATTR = ['dirty_state',\n",
    "                    'dirty_city',\n",
    "                    'dirty_zip']\n",
    "TARGET = ['rent']\n",
    "OMIT = ['intended_state', 'intended_city', 'intended_zip']\n",
    "TRAIN_FILENAME = 'rent_data_low_error.csv'\n",
    "VALID_FILENAME = 'rent_data_low_error_validation.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Load dataset into one vocab for all columns, integer-encoded\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "valid_df = pd.read_csv(VALID_FILENAME)\n",
    "'''unique_values = np.concatenate(([train_df[cat_attr].astype(str).unique()\n",
    "                                 for cat_attr in CATEGORICAL_ATTR]), axis=None)\n",
    "keys = pd.Categorical(unique_values).codes\n",
    "category_dict = dict(zip(keys, unique_values))\n",
    "train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "valid_df[CATEGORICAL_ATTR] = valid_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "for o in OMIT:\n",
    "    train_df.pop(o)\n",
    "    valid_df.pop(o)'''\n",
    "    \n",
    "# Load dataset into separate vocabularies, integer-encoded\n",
    "unique_values = [train_df[cat_attr].astype(str).unique() for cat_attr in CATEGORICAL_ATTR]\n",
    "keys = [pd.Categorical(u).codes for u in unique_values]\n",
    "category_dict = []\n",
    "for i in range(len(keys)):\n",
    "    category_dict.append(dict(zip(keys[i], unique_values[i])))\n",
    "    train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values[i], keys[i])\n",
    "for o in OMIT:\n",
    "    train_df.pop(o)\n",
    "    valid_df.pop(o)\n",
    "# Generate labels, i.e. we label all observations as positive samples\n",
    "train_target = np.ones(train_df.shape[0])\n",
    "valid_target = np.ones(valid_df.shape[0])\n",
    "\n",
    "# Load data into model, single vocab\n",
    "# vocabulary_size = len(unique_values)\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(vocabulary_size ** 0.25))\n",
    "# model = get_w2v_model(len(CATEGORICAL_ATTR), vocabulary_size, adaptive_embedding_dim)\n",
    "\n",
    "# multi vocab\n",
    "vocabulary_size = [len(u) for u in unique_values]\n",
    "# adaptive_embedding_dim = min(EMBEDDING_DIM, int(sum(vocabulary_size)**0.25))\n",
    "adaptive_embedding_dim = 10\n",
    "model = get_w2v_multi_vocab_model(len(CATEGORICAL_ATTR), len(TARGET), vocabulary_size, adaptive_embedding_dim)\n",
    "train_np = [train_df[cat_attr].to_numpy() for cat_attr in CATEGORICAL_ATTR]\n",
    "train_np.append(train_df[TARGET].to_numpy())\n",
    "\n",
    "history = model.fit(x=train_np,\n",
    "                    y=train_target,\n",
    "                    batch_size=64,\n",
    "                    epochs=100,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save embeddings from one vocab\n",
    "'''e = model.layers[3]\n",
    "weights = e.get_weights()[0]\n",
    "out_v = io.open('w2v_vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('w2v_meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for i in range(0, vocabulary_size):\n",
    "    vec = weights[i]\n",
    "    out_m.write(category_dict[i] + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Save for multiple vocabs\n",
    "tsv_name = '{}_{}.tsv'\n",
    "for i, cat_attr in enumerate(CATEGORICAL_ATTR, start=0):\n",
    "    e = model.layers[3 + i]\n",
    "    weights = e.get_weights()[0]\n",
    "    out_v = io.open(tsv_name.format(cat_attr, 'vec'), 'w', encoding='utf-8')\n",
    "    out_m = io.open(tsv_name.format(cat_attr, 'meta'), 'w', encoding='utf-8')\n",
    "    \n",
    "    for j in range(0, vocabulary_size[i]):\n",
    "        vec = weights[j]\n",
    "        out_m.write(category_dict[i][j] + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "'''\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embedding unpatched dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['catalog_id',\n",
    "                    'article_id',\n",
    "                    'destination',\n",
    "                    'lower_bound',\n",
    "                    'currency',\n",
    "                    'unit',\n",
    "                    'set_id',\n",
    "                    'duplicate_set_rating']\n",
    "TARGET = 'ek_amount'\n",
    "OMIT = ['vk_amount', 'currency', 'destination']\n",
    "FILENAME = 'DE_unpatched_100000.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(FILENAME)\n",
    "for cat_attr in CATEGORICAL_ATTR:\n",
    "    df[cat_attr] = pd.Categorical(df[cat_attr])\n",
    "    df[cat_attr] = df[cat_attr].cat.codes\n",
    "for o in OMIT:\n",
    "    df.pop(o)\n",
    "target = df.pop(TARGET)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))\n",
    "train_dataset = dataset.shuffle(len(df)).batch(1)\n",
    "vocab_size = sum([len(df[cat_attr].unique()) for cat_attr in CATEGORICAL_ATTR if cat_attr not in OMIT])\n",
    "'''for feat, targ in dataset.take(5):\n",
    "    print('Features: {}, Target: {}'.format(feat, targ))\n",
    "'''\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, EMBEDDING_DIM))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "dump_history(history, 'hist.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_ATTR = ['dirty_state',\n",
    "                    'dirty_city',\n",
    "                    'dirty_zip']\n",
    "TARGET = 'rent'\n",
    "OMIT = ['intended_state', 'intended_city', 'intended_zip']\n",
    "TRAIN_FILENAME = 'rent_data_low_error.csv'\n",
    "VALID_FILENAME = 'rent_data_low_error_validation.csv'\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILENAME)\n",
    "unique_values = np.concatenate(([train_df[cat_attr].astype(str).unique()\n",
    "                                 for cat_attr in CATEGORICAL_ATTR]), axis=None)\n",
    "keys = pd.Categorical(unique_values).codes\n",
    "category_dict = dict(zip(keys, unique_values))\n",
    "train_df[CATEGORICAL_ATTR] = train_df[CATEGORICAL_ATTR].astype(str).replace(unique_values, keys)\n",
    "\n",
    "for o in OMIT:\n",
    "    train_df.pop(o)\n",
    "    \n",
    "hihi = train_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tsv_name = '{}_{}.tsv'\n",
    "for i, cat_attr in enumerate(CATEGORICAL_ATTR, start=0):\n",
    "    e = model.get_layer(index=3+i)\n",
    "    weights = e.get_weights()[0]\n",
    "    out_v = io.open(tsv_name.format(cat_attr, 'test_vec'), 'w', encoding='utf-8')\n",
    "    out_m = io.open(tsv_name.format(cat_attr, 'test_meta'), 'w', encoding='utf-8')\n",
    "    \n",
    "    for j in range(0, vocabulary_size[i]):\n",
    "        vec = weights[j]\n",
    "        out_m.write(category_dict[i][j] + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cbow_model(input_length, voc_size, e_dim):\n",
    "    assert input_length > 1, 'Input length must be greater than 1, current: %i' % input_length \n",
    "    word_input_list = []\n",
    "    word_input_name = 'word_input_{}'\n",
    "    for i in range(input_length):\n",
    "        input_i = keras.layers.Input(shape=(1, ), name=word_input_name.format(i))\n",
    "        word_input_list.append(input_i)\n",
    "    numerical_input = keras.layers.Input(shape=(1, ))\n",
    "    \n",
    "    embedding = keras.layers.Embedding(input_dim=voc_size, \n",
    "                                       output_dim=e_dim, \n",
    "                                       input_length=input_length,\n",
    "                                       name='embedding')\n",
    "    word_reshaped_list = []\n",
    "    word_reshaped_name = 'encode_reshape_{}'\n",
    "    for i in range(input_length):\n",
    "        encoded_i = embedding(word_input_list[i])\n",
    "        reshape_i = keras.layers.Reshape((e_dim, ), name=word_reshaped_name.format(i))(encoded_i)\n",
    "        word_reshaped_list.append(reshape_i)\n",
    "    \n",
    "    dot_sim_list = []\n",
    "    dot_sim_name = 'dot_sim_{}_{}'\n",
    "    for i in range(input_length-1):\n",
    "        for j in range(i+1, input_length):\n",
    "            dot_sim_i = keras.layers.dot([word_reshaped_list[i], word_reshaped_list[j]] , \n",
    "                                         axes=1, \n",
    "                                         normalize=True,\n",
    "                                         name=dot_sim_name.format(i, j))\n",
    "            dot_sim_list.append(dot_sim_i)\n",
    "    merge_sim = keras.layers.concatenate(dot_sim_list, axis=1) if len(dot_sim_list) > 1 else dot_sim_list[0]\n",
    "    merge_final = keras.layers.concatenate([merge_sim, numerical_input], axis=1)\n",
    "    output = keras.layers.Dense(units=1, activation='sigmoid')(merge_final)\n",
    "    \n",
    "    word_input_list.append(numerical_input)\n",
    "    m = keras.Model(inputs=word_input_list, outputs=output, name='cbow_model')\n",
    "    m.summary()\n",
    "    return m    \n",
    "\n",
    "hihi = get_cbow_model(5, 200, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}